{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit submission analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we'll be predicting the number of upvotes a submission in a subreddit received, based on their title. Because upvotes are an indicator of popularity, we'll discover which types of articles tend to be the most popular.\n",
    "\n",
    "The submissions are based on the hot section in the subreddit which sorts the submission based on the submission time and upvotes.The program iterates through multiple subreddits to find the popular submission. As a sample we have considered the two subreddit - python and politics.\n",
    "\n",
    "The program runs Linear regression and ridge regression to find out the predicted value of upvotes for the testing set.\n",
    "\n",
    "for the regression,  Each word in title is tokenized and is converted to a numerical representations. Only the words that appear more than the 5 times are considered as feattures to avoid overfitting. Only the words that appear less than 50 times are considered as they might mostly be stop word and might skew the result.\n",
    "\n",
    "In the end we should be able to see the top posts in each of the subreddit.\n",
    "\n",
    "The program can be improved by - \n",
    "1) finding out the proper upper cut off and lower cut of point for considering the features.\n",
    "2) Increasing the dataset set size\n",
    "3) Adding features like headline lenght and word length\n",
    "4) Using other algorithms like random forest\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis for subreddit:python\n",
      " The mean of up_votes is 64.7171717172 and the std dev is 289.271641295 The average predicted upvotes using Linear regression is 227.914981362away from real value\n",
      " The mean of up_votes is 64.7171717172 and the std dev is 289.271641295 The average predicted upvotes using ridge regression is 349.781565327away from real value\n",
      "Analysis for subreddit:politics\n",
      " The mean of up_votes is 2070.17085427 and the std dev is 5269.87849331 The average predicted upvotes using Linear regression is 8706.18086051away from real value\n",
      " The mean of up_votes is 2070.17085427 and the std dev is 5269.87849331 The average predicted upvotes using ridge regression is 6341.22223522away from real value\n"
     ]
    }
   ],
   "source": [
    "client_id = 'xxxxxA'\n",
    "secret = 'xxxxx'\n",
    "\n",
    "#-----------------Import Modules-----------------#\n",
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#-----------------Define variables-----------------#\n",
    "subreddits = ['python', 'politics']\n",
    "data_list = {}\n",
    "rows = 100\n",
    "word_count_lower_limit = 5\n",
    "word_count_upper_limit = 50\n",
    "subreddit_top_5 = []\n",
    "num_top_posts = 5\n",
    "\n",
    "\n",
    "\n",
    "#-----------------connect to reddit api using Praw wrapper-----------------#\n",
    "reddit = praw.Reddit(client_id= client_id, client_secret = secret, username = '',\n",
    "                     password  ='PuzzledTarget', user_agent ='Incubator_project')\n",
    "\n",
    "#-----------------extract data from the subreddit -----------------#\n",
    "for one_subreddit in subreddits:\n",
    "    subreddit = reddit.subreddit(one_subreddit)\n",
    "    \n",
    "    tokenized_title = []\n",
    "    unique_tokens = []\n",
    "    single_token =[]\n",
    "\n",
    "    # extracting from the hot tab ( Time and votes are considered for hot rating)\n",
    "    hot_python = subreddit.hot(limit = rows)\n",
    "\n",
    "\n",
    "    for submission in hot_python:\n",
    "        if not submission.stickied:\n",
    "            if submission.id not in data_list :\n",
    "                created_time = (datetime.utcfromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "                data_list[submission.id] =[submission.title, submission.ups, submission.downs, created_time, submission.upvote_ratio]\n",
    "            else:\n",
    "                raise ValueError('duplicate id found')\n",
    "                \n",
    "    #-----------------Create a pandas DataFrame out of the dictionary -----------------#\n",
    "    columns = ['title' , 'ups' , 'downs', 'create_time' , 'upvote_ratio' ]\n",
    "    reddit_submission = pd.DataFrame.from_dict(data_list, orient = 'index' , columns = columns)\n",
    "    print('Analysis for subreddit:' + one_subreddit)\n",
    "    \n",
    "    if reddit_submission[reddit_submission.isnull().any(axis=1)].shape[0] > 0:\n",
    "        raise ValueError('null values observed')\n",
    "    \n",
    "    \n",
    "    # covert each title into a numerical repesentation\n",
    " \n",
    "\n",
    "    for item in reddit_submission['title']:\n",
    "        tokenized_title.append(item.split(\" \"))\n",
    "        \n",
    "    # lowercase all the items and removing punctuations\n",
    "    punctuation = [\",\", \":\", \";\", \".\", \"'\", '\"', \"â€™\", \"?\", \"/\", \"-\", \"+\", \"&\", \"(\", \")\", \"|\" , \">\" , \"<\" , \"[\" , \"]\" , \"-\"]\n",
    "\n",
    "    clean_tokenized = []\n",
    "\n",
    "    for item in tokenized_title:\n",
    "        tokens = []\n",
    "        for token in item:\n",
    "            token = token.lower()\n",
    "            for punc in punctuation:\n",
    "                token = token.replace(punc, \"\")\n",
    "            if token != \"\":\n",
    "                tokens.append(token)\n",
    "        clean_tokenized.append(tokens)\n",
    "        \n",
    "    # find all the unique token in clean_tokenised and assign the result to unique tokenize. Any token occuring\n",
    "    # only once is eleminated from unique token and added to single_token\n",
    "        for item in clean_tokenized:\n",
    "            for element in item:\n",
    "                if element not in  single_token:\n",
    "                    single_token.append(element)\n",
    "                elif element in single_token and element not in unique_tokens:\n",
    "                    unique_tokens.append(element)\n",
    "\n",
    "\n",
    "    # initialising DataFrame to hold the numeric values for each token \n",
    "    counts =  pd.DataFrame(0, index = np.arange(len(clean_tokenized)) , columns = unique_tokens)\n",
    "    \n",
    "    \n",
    "    # for the counts dataframe ,set the index and for each word increment the value count\n",
    "    for i,item in enumerate(clean_tokenized):\n",
    "        for element in item:\n",
    "            if element in unique_tokens:\n",
    "                counts.iloc[i][element] +=1 \n",
    "            else:\n",
    "                continue\n",
    "\n",
    "\n",
    "    # Features or words occuring too few times will result in overfillting These feature will \n",
    "    # probably correlates differently with upvote in training set and testing set.Features or \n",
    "    # words occuring too many times will also cause issue (stopwords - such as 'and','or' etc). \n",
    "    # They do not add any information to the model.\n",
    "    # After having a look at the word_count distribution, \n",
    "    # to make the model better we reduce the feature by removing words that occur less than \n",
    "    # 5(word_count_lower_limit) times or more than 50 (word_count_upper_limit) times.\n",
    "    \n",
    "    word_counts = counts.sum(axis =0)\n",
    "    counts = counts.loc[:,(word_counts >= word_count_lower_limit) & (word_counts <= word_count_upper_limit)]\n",
    "    counts.shape\n",
    "    \n",
    "    #Now we will split the data into 2 sets. Test and train to evaluate the algorithm effectively. \n",
    "    #we will select 20% of our rows for test and 80% of our rows for training. \n",
    "    \n",
    "    #-------------Linear Regression----------------------------------------------------\n",
    "    \n",
    "    #We will use linear regression algorithm.\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(counts, reddit_submission[\"ups\"], test_size=0.2, random_state=1)\n",
    "\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    predictions = lr.predict(X_test)\n",
    "    \n",
    "    #lets us calculate the MSE (mean square error associated with our predictions)\n",
    "    mse_lr = ((predictions - y_test)**2).sum()/len(predictions)\n",
    "    mse_lr_std_error = mse_lr**(1/2)\n",
    "    \n",
    "    # mean of up_votes in the original dataset\n",
    "    ups_mean = reddit_submission[\"ups\"].describe()[1]\n",
    "    ups_std_dev = reddit_submission[\"ups\"].describe()[2]\n",
    "\n",
    "\n",
    "    print (' The mean of up_votes is ' + str(ups_mean) + ' and the std dev is ' + str(ups_std_dev)\n",
    "           + ' The average predicted upvotes using Linear regression is ' + str(mse_lr_std_error) \n",
    "           + 'away from real value')\n",
    "        \n",
    "    #-------------Ridge Regression----------------------------------------------------\n",
    "    # We will use ridge regression to predict the up_votes \n",
    "    \n",
    "    train_rows = int(counts.shape[0]* .8)\n",
    "    # Set a seed to get the same \"random\" shuffle every time.\n",
    "    random.seed(1)\n",
    "\n",
    "    # Shuffle the indices for the matrix.\n",
    "    indices = list(range(counts.shape[0]))\n",
    "    random.shuffle(indices)\n",
    "\n",
    "    \n",
    "    # Create train and test sets.\n",
    "    X_train_ridge = counts.loc[indices[:train_rows], :]\n",
    "    X_test_ridge = counts.loc[indices[train_rows:], :]\n",
    "    y_train_ridge = reddit_submission[\"ups\"].iloc[indices[:train_rows]]\n",
    "    y_test_ridge = reddit_submission[\"ups\"].iloc[indices[train_rows:]]\n",
    "    X_train_ridge = np.nan_to_num(X_train_ridge)\n",
    "\n",
    "    # Run the regression and generate predictions for the test set.\n",
    "    reg = Ridge(alpha=.1)\n",
    "    reg.fit(X_train_ridge, y_train_ridge)\n",
    "    predictions_ridge = reg.predict(X_test_ridge)\n",
    "    \n",
    "    mse_ridge = ((predictions_ridge - y_test_ridge)**2).sum()/len(y_test_ridge)\n",
    "    mse_ridge_std_error = mse_ridge**(1/2)\n",
    "    \n",
    "\n",
    "    print (' The mean of up_votes is ' + str(ups_mean) + ' and the std dev is ' + str(ups_std_dev)\n",
    "           + ' The average predicted upvotes using ridge regression is ' + str(mse_ridge_std_error) \n",
    "           + 'away from real value')\n",
    "                \n",
    "                \n",
    "    # selecting the model with the lesser standard error for\n",
    "    \n",
    "    if mse_ridge_std_error < mse_lr_std_error:\n",
    "        \n",
    "        predictions_ridge = pd.DataFrame(data = predictions_ridge , index = X_test_ridge.index )\n",
    "        reddit_submission.index = counts.index\n",
    "        reddit_submission.loc[X_test_ridge.index,:]\n",
    "        reddit_predictions = pd.merge(reddit_submission, predictions_ridge, left_index = True, right_index = True)\n",
    "       \n",
    "    else:\n",
    "        predictions = pd.DataFrame(data = predictions , index = X_test.index )\n",
    "        reddit_submission.index = counts.index\n",
    "        reddit_submission.loc[X_test.index,:]\n",
    "        reddit_predictions = pd.merge(reddit_submission, predictions, left_index = True, right_index = True)\n",
    "    \n",
    "    reddit_predictions['predicted_ups'] = reddit_predictions[0] \n",
    "        \n",
    "    # top 5 posts that have maximum predicted up_votes\n",
    "    top_5 = reddit_predictions.sort_values('predicted_ups' , ascending = False)['title'].head(num_top_posts)\n",
    "    subreddit_top_5.append((one_subreddit, top_5))\n",
    "     \n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('python', 31    Download Instagram Photos and Videos Based on ...\n",
       "  78    How to distribute Python Slack bot running on ...\n",
       "  92        Need help with PDF split based on the content\n",
       "  77    Control a web page that is signed in to your a...\n",
       "  54                How to average across many wav files?\n",
       "  Name: title, dtype: object),\n",
       " ('politics', 99     Trump launches unprovoked attack on beloved bl...\n",
       "  184    Baltimore stands up for its city after Trump t...\n",
       "  145    U.S. Senator Patty Murray supports Trump impea...\n",
       "  195    Steph Curry Forcefully Responds to Donald Trum...\n",
       "  189    'His feed is the most hate-filled, racist, and...\n",
       "  Name: title, dtype: object)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#subreddit with top tiles\n",
    "subreddit_top_5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
